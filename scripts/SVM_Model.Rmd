---
title: "SVM_Model"
author: "Ivan Li"
date: "2024-11-14"
output: html_document
---

```{r, warning=FALSE, message=FALSE}
# Import libraries
library(dplyr)
library(tidyr)
library(tidymodels)
library(knitr)
library(tidyverse)
library(tidytext)
library(textstem)
library(rvest)
library(qdapRegex)
library(stopwords)
library(tokenizers)
library(kableExtra)
library(kernlab)
library(Matrix)
library(sparsesvd)
library(caret)
library(irlba)
library(factoextra)
```


```{r}
# NLP Functions, remove and tidy up later
nlp_fn_multi <- function(parse_data.out){
  out <- parse_data.out %>% 
    unnest_tokens(output = token, 
                  input = text_clean, 
                  token = 'words',
                  stopwords = str_remove_all(stop_words$word, 
                                             '[[:punct:]]')) %>%
    mutate(token.lem = lemmatize_words(token)) %>%
    filter(str_length(token.lem) > 2) %>%
    count(.id, mclass, token.lem, name = 'n') %>%
    bind_tf_idf(term = token.lem, 
                document = .id,
                n = n) %>%
    pivot_wider(id_cols = c('.id', 'mclass'),
                names_from = 'token.lem',
                values_from = 'tf_idf',
                values_fill = 0)
  return(out)
}
```


```{r}
# Import training data
load("~/GitHub/module-2-group15/data/prelim1_data.RData")

head(prelim1_data)
```


```{r}
# tokenize the data
tokenized_data <- nlp_fn_multi(prelim1_data)

# Set class as a factor
tokenized_data$mclass <- as.factor(tokenized_data$mclass)

head(tokenized_data)
```

```{r}
# Method 2 testing (TODO)

# split data

split = initial_split(tokenized_data, prop = 0.70, strata = mclass)
training = training(split)
testing = testing(split)

# v fold cross validation
text_folds = vfold_cv(data=tokenized_data, v=3, strata = mclass)

# create recipe with pca
recipe = recipe(mclass ~ ., data=training %>% select(-.id)) %>%
  step_pca(all_predictors(), num_comp = 100) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_predictors(), -all_outcomes()) %>%
  step_nzv(all_predictors(), -all_outcomes())
  
# run svm model on training set
svm_model <- svm_rbf(cost = tune(), rbf_sigma = tune()) %>% 
  set_mode("classification") %>% 
  set_engine("kernlab")

# Define SVM workflow
svm_wkflow <- workflow() %>% 
  add_model(svm_model) %>% 
  add_recipe(recipe)

# Define SVM tuning grid
svm_tune_grid <- grid_random(cost(), rbf_sigma(), size = 10)

# Store model results
#svm_tuned <- tune_grid(
#  object=svm_wkflow,
#  resamples = text_folds,
#  grid = svm_tune_grid,
#  metrics = metric_set(roc_auc)
#)

```


```{r}
# Split labels and predictors

# Isolate predictors
train_preds <- tokenized_data %>%
  select(-.id, -mclass)

# Isolate Labels
train_labels <- tokenized_data %>%
  select(.id, mclass)

# View Predictors
head(train_preds)

# View labels
head(train_labels)
```


```{r}
# Perform PCA on training set
# This section is commented out for time convenience 
# Results are stored as 'principal_components.RData'

# Ensure all columns in train_preds are numeric
# train_preds <- as.data.frame(lapply(train_preds, as.numeric))  # Convert all columns to numeric

# Step 2: Remove near-zero variance columns to reduce dimensionality
# nzv <- nearZeroVar(train_preds, saveMetrics = TRUE)
# train_preds_reduced <- train_preds[, !nzv$nzv]  # Keep only columns with non-zero variance

# Convert to a matrix for PCA
# train_matrix <- as.matrix(train_preds_reduced)

# Perform PCA, find 50 components
# k <- 50 
# pca_irlba <- irlba(train_matrix, nv = k)

# Extract and store the principal components
# principal_components <- pca_irlba$u %*% diag(pca_irlba$d)
```

```{r}
# save the principal components
# save(principal_components, file ='principal_components.RData')
```

```{r}
# load the principal components
load("~/GitHub/module-2-group15/scripts/principal_components.RData")
```


```{r}
# Convert principal components to data frame
pc_df <- as.data.frame(principal_components)

# Combine labels with new projected matrix
projected_train_set <- train_labels %>%
  transmute(mclass = factor(mclass)) %>%
  bind_cols(pc_df)
```

```{r}
# v-fold cross validation
text_folds <- vfold_cv(data=projected_train_set, v=3, strata=mclass)
```


```{r}
# Create a recipe for the model
text_recipe <- recipe(mclass ~ ., data=projected_train_set) %>%
  step_dummy(all_nominal_predictors()) %>%  
  step_normalize(all_predictors()) %>% 
  step_zv(all_predictors()) %>%  
  step_center(all_predictors()) %>% 
  step_scale(all_predictors()) %>% 
  step_nzv(all_predictors()) 
```

```{r}
# Define SVM model
svm_model <- svm_rbf(cost = tune(), rbf_sigma = tune()) %>% 
  set_mode("classification") %>% 
  set_engine("kernlab")

# Define SVM workflow
svm_wkflow <- workflow() %>% 
  add_model(svm_model) %>% 
  add_recipe(text_recipe)

# Define SVM tuning grid
svm_tune_grid <- grid_random(cost(), rbf_sigma(), size = 10)

# Store model results
svm_tuned <- tune_grid(
  object=svm_wkflow,
  resamples = text_folds,
  grid = svm_tune_grid,
  metrics = metric_set(roc_auc)
)
```

```{r}
# save svm model
# write_rds(svm_tuned, file = "models/svm.rds")
```

```{r}
# load svm model
svm_tuned <- read_rds(file = "models/svm.rds")

best_svm_model <- collect_metrics(svm_tuned) %>%
  filter(.metric == "roc_auc") %>%
  arrange(desc(mean)) %>%
  slice(1)

best_svm_model
```

```{r}
# SVM autoplot
autoplot(svm_tuned, metric = 'roc_auc') + 
  ggtitle("SVM Classifier ROC AUC Tuning Results") + 
  ylab("Area Under ROC Curve") + 
  theme_minimal()
```


```{r}
# Classification accuracy visualization (TODO)

# have to take the testing set, tokenize, and get into pca form
# use this pca form testing set, run trained model on it.

final_svm_model <- finalize_workflow(svm_wkflow, best_svm_model)
final_svm_model_train <- fit(final_svm_model, projected_train_set)

# final_svm_model_test <- augment(final_svm_model_train, new_data = claims_test)
# final_svm_model_test
```
