## this script contains functions for preprocessing
## claims data; intended to be sourced
require(tidyverse)
require(tidytext)
require(textstem)
require(rvest)
require(qdapRegex)
require(stopwords)
require(tokenizers)
# function to parse html and clean text
parse_fn <- function(.html){
read_html(.html) %>%
html_elements('p, h1, h2') %>%
html_text2() %>%
str_c(collapse = ' ') %>%
rm_url() %>%
rm_email() %>%
str_remove_all('\'') %>%
str_replace_all(paste(c('\n',
'[[:punct:]]',
'nbsp',
'[[:digit:]]',
'[[:symbol:]]'),
collapse = '|'), ' ') %>%
str_replace_all("([a-z])([A-Z])", "\\1 \\2") %>%
tolower() %>%
str_replace_all("\\s+", " ")
}
# function to apply to claims data
parse_data <- function(.df){
out <- .df %>%
filter(str_detect(text_tmp, '<!')) %>%
rowwise() %>%
mutate(text_clean = parse_fn(text_tmp)) %>%
unnest(text_clean)
return(out)
}
nlp_fn <- function(parse_data.out){
out <- parse_data.out %>%
unnest_tokens(output = token,
input = text_clean,
token = 'words',
stopwords = str_remove_all(stop_words$word,
'[[:punct:]]')) %>%
mutate(token.lem = lemmatize_words(token)) %>%
filter(str_length(token.lem) > 2) %>%
count(.id, bclass, token.lem, name = 'n') %>%
bind_tf_idf(term = token.lem,
document = .id,
n = n) %>%
pivot_wider(id_cols = c('.id', 'bclass'),
names_from = 'token.lem',
values_from = 'tf_idf',
values_fill = 0)
return(out)
}
nlp_fn_multi <- function(parse_data.out){
out <- parse_data.out %>%
unnest_tokens(output = token,
input = text_clean,
token = 'words',
stopwords = str_remove_all(stop_words$word,
'[[:punct:]]')) %>%
mutate(token.lem = lemmatize_words(token)) %>%
filter(str_length(token.lem) > 2) %>%
count(.id, mclass, token.lem, name = 'n') %>%
bind_tf_idf(term = token.lem,
document = .id,
n = n) %>%
pivot_wider(id_cols = c('.id', 'mclass'),
names_from = 'token.lem',
values_from = 'tf_idf',
values_fill = 0)
return(out)
}
nlp_fn_bigrams <- function(parse_data.out){
out <- parse_data.out %>%
unnest_tokens(output = bigram,
input = text_clean,
token = 'ngrams',
n = 2,
stopwords = str_remove_all(stop_words$word,
'[[:punct:]]')) %>%
separate(bigram, into = c("word1", "word2"), sep = " ") %>%
unite("bigram", word1, word2, sep = " ") %>%
count(.id, bclass, bigram, name = 'n') %>%
bind_tf_idf(term = bigram,
document = .id,
n = n) %>%
pivot_wider(id_cols = c('.id', 'bclass'),
names_from = 'bigram',
values_from = 'tf_idf',
values_fill = 0)
return(out)
}
# Projection functions
projection_fn <- function(.dtm, .prop){
# coerce feature matrix to sparse
dtm_mx <- .dtm %>%
as.matrix() %>%
as('sparseMatrix')
# compute svd
svd_out <- svd(dtm_mx) # svd function: singular value decomposition
# select number of projections
var_df <- tibble(var = svd_out$d^2) %>%
mutate(pc = row_number(),
cumulative = cumsum(var)/sum(var))
n_pc <- which.min(var_df$cumulative < .prop)
# extract loadings
loadings <- svd_out$v[, 1:n_pc] %>% as.matrix()
# extract scores
scores <- (dtm_mx %*% svd_out$v[, 1:n_pc]) %>% as.matrix()
# adjust names
colnames(loadings) <- colnames(scores) <- paste('pc', 1:n_pc, sep = '')
# output
out <- list(n_pc = n_pc,
var = var_df,
projection = loadings,
data = as_tibble(scores))
return(out)
}
reproject_fn <- function(.dtm, .projection_fn_out){
as_tibble(as.matrix(.dtm) %*% .projection_fn_out$projection)
}
prelim1_data = parse_data(claims_raw)
# Import libraries
library(dplyr)
library(tidyr)
library(tidymodels)
library(knitr)
library(tidyverse)
library(tidytext)
library(textstem)
library(rvest)
library(qdapRegex)
library(stopwords)
library(tokenizers)
library(kableExtra)
library(kernlab)
library(Matrix)
library(sparsesvd)
# Import training data
load("~/GitHub/module-2-group15/data/prelim1_data.RData")
head(prelim1_data)
# tokenize the data and select relevant columns
tokenized_data <- nlp_fn_multi(prelim1_data)
# Set class as a factor
tokenized_data$mclass <- as.factor(tokenized_data$mclass)
head(tokenized_data)
# Coerce into sparce DTM Matrix
train_dtm <- tokenized_data %>%
select(-.id, -mclass) %>%
as.matrix() %>%
as("sparseMatrix")
# View the proportion of words that have weight 0
1 - nnzero(train_dtm)/length(train_dtm)
# Split labels and predictors
# train_dtm is the isolated predictors as DTM
# Split labels from predictors
train_labels <- tokenized_data %>%
select(.id, mclass)
# View labels
head(train_labels)
# PCA
# find projections based on training data
dense_dtm <- as.matrix(train_dtm)
# View how many components were used
dense_dtm
# Split labels and predictors
# Isolate predictors
train_dtm <- tokenized_data %>%
select(-.id, -mclass)
# Isolate Labels
train_labels <- tokenized_data %>%
select(.id, mclass)
# View Predictors
head(train_dtm)
# View labels
head(train_labels)
# Split labels and predictors
# Isolate predictors
train_preds <- tokenized_data %>%
select(-.id, -mclass)
# Isolate Labels
train_labels <- tokenized_data %>%
select(.id, mclass)
# View Predictors
head(train_preds)
# View labels
head(train_labels)
install.packages("irlba")
install.packages("factoextra")
# Load necessary libraries
library(caret)
library(irlba)
library(factoextra)
# Step 1: Ensure all columns in train_preds are numeric
train_preds <- as.data.frame(lapply(train_preds, as.numeric))  # Convert all columns to numeric
# Step 2: Remove near-zero variance columns to reduce dimensionality
nzv <- nearZeroVar(train_preds, saveMetrics = TRUE)
# Load necessary libraries
library(caret)
library(irlba)
library(factoextra)
# Step 1: Ensure all columns in train_preds are numeric
train_preds <- as.data.frame(lapply(train_preds, as.numeric))  # Convert all columns to numeric
# Step 2: Remove near-zero variance columns to reduce dimensionality
nzv <- nearZeroVar(train_preds, saveMetrics = TRUE)
train_preds_reduced <- train_preds[, !nzv$nzv]  # Keep only columns with non-zero variance
# Step 3: Convert to a matrix for PCA
train_matrix <- as.matrix(train_preds_reduced)
# Step 4: Perform PCA using the irlba package (efficient for large datasets)
k <- 50  # Number of principal components to compute
pca_irlba <- irlba(train_matrix, nv = k)
# Step 5: Extract and use the principal components
# Principal components matrix (documents in rows, PCs in columns)
principal_components <- pca_irlba$u %*% diag(pca_irlba$d)
# Explained variance for each component
explained_variance <- (pca_irlba$d^2) / sum(pca_irlba$d^2)
cumulative_variance <- cumsum(explained_variance)
principal_components
nrow(principal_components)
ncol(principal_components)
as.data.frame(principal_components)
pc_df <- as.data.frame(principal_components)
# Combine labels with new projected matrix
projected_train_set <- train_labels %>%
transmute(mclass = factor(mclass)) %>%
bind_cols(pc_df)
projected_train_set
save(principal_components, file = 'data/principal_components.RData')
save(principal_components, file ='data/principal_components.RData')
save(principal_components, file ='data/principal_components.RData')
save(principal_components, file ='principal_components.RData')
# Create a recipe for the model
text_recipe <- recipe(mclass ~ ., data=projected_train_set %>% select(-.id)) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_predictors()) %>%
step_zv(all_predictors()) %>%
step_center(all_predictors()) %>%
step_scale(all_predictors()) %>%
step_nzv(all_predictors())
projected_train_set
# Create a recipe for the model
text_recipe <- recipe(mclass ~ ., data=projected_train_set) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_predictors()) %>%
step_zv(all_predictors()) %>%
step_center(all_predictors()) %>%
step_scale(all_predictors()) %>%
step_nzv(all_predictors())
# Define SVM model
svm_model <- svm_rbf(cost = tune(), rbf_sigma = tune()) %>%
set_mode("classification") %>%
set_engine("kernlab")
# Define SVM workflow
svm_wkflow <- workflow() %>%
add_model(svm_model) %>%
add_recipe(text_recipe)
# Define SVM tuning grid
svm_tune_grid <- grid_random(cost(), rbf_sigma(), size = 10)
# Store model results
svm_tuned <- tune_grid(
object=svm_wkflow,
resamples = text_folds,
grid = svm_tune_grid,
metrics = metric_set(roc_auc)
)
# v-fold cross validation
text_folds <- vfold_cv(data=projected_train_set, v=3, strata=mclass)
# Create a recipe for the model
text_recipe <- recipe(mclass ~ ., data=projected_train_set) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_predictors()) %>%
step_zv(all_predictors()) %>%
step_center(all_predictors()) %>%
step_scale(all_predictors()) %>%
step_nzv(all_predictors())
head(projected_train_set)
# Define SVM model
svm_model <- svm_rbf(cost = tune(), rbf_sigma = tune()) %>%
set_mode("classification") %>%
set_engine("kernlab")
# Define SVM workflow
svm_wkflow <- workflow() %>%
add_model(svm_model) %>%
add_recipe(text_recipe)
# Define SVM tuning grid
svm_tune_grid <- grid_random(cost(), rbf_sigma(), size = 10)
# Store model results
svm_tuned <- tune_grid(
object=svm_wkflow,
resamples = text_folds,
grid = svm_tune_grid,
metrics = metric_set(roc_auc)
)
# save svm model
write_rds(svm_tune_res, file = "models/svm.rds")
# save svm model
write_rds(svm_tuned, file = "models/svm.rds")
# load svm model
svm_tuned <- read_rds(file = "models/svm.rds")
best_svm_model <- collect_metrics(svm_tuned) %>%
filter(.metric == "roc_auc") %>%
arrange(desc(mean)) %>%
slice(1)
best_svm_model
# SVM autoplot
autoplot(svm_tuned, metric = 'roc_auc') +
ggtitle("SVM Classifier ROC AUC Tuning Results") +
ylab("Area Under ROC Curve") +
theme_minimal()
# Classification accuracy visualization (TODO)
final_svm_model <- finalize_workflow(svm_wkflow, best_svm_model)
final_svm_model_train <- fit(final_svm_model, tokenized_data)
# Classification accuracy visualization (TODO)
final_svm_model <- finalize_workflow(svm_wkflow, best_svm_model)
final_svm_model_train <- fit(final_svm_model, projected_train_set)
final_svm_model_test <- augment(final_svm_model_train, new_data = claims_test)
# Import testing data
load("~/GitHub/module-2-group15/data/claims-test.RData")
head(claims_test)
# Classification accuracy visualization (TODO)
final_svm_model <- finalize_workflow(svm_wkflow, best_svm_model)
final_svm_model_train <- fit(final_svm_model, projected_train_set)
final_svm_model_test <- augment(final_svm_model_train, new_data = claims_test)
# Classification accuracy visualization (TODO)
final_svm_model <- finalize_workflow(svm_wkflow, best_svm_model)
final_svm_model_train <- fit(final_svm_model, projected_train_set)
# final_svm_model_test <- augment(final_svm_model_train, new_data = claims_test)
# final_svm_model_test
final_svm_model_train
