<<<<<<< HEAD
## this script contains functions for preprocessing
## claims data; intended to be sourced
require(tidyverse)
require(tidytext)
require(textstem)
require(rvest)
require(qdapRegex)
require(stopwords)
require(tokenizers)
# function to parse html and clean text
parse_fn <- function(.html){
read_html(.html) %>%
html_elements('p, h1, h2') %>%
html_text2() %>%
str_c(collapse = ' ') %>%
rm_url() %>%
rm_email() %>%
str_remove_all('\'') %>%
str_replace_all(paste(c('\n',
'[[:punct:]]',
'nbsp',
'[[:digit:]]',
'[[:symbol:]]'),
collapse = '|'), ' ') %>%
str_replace_all("([a-z])([A-Z])", "\\1 \\2") %>%
tolower() %>%
str_replace_all("\\s+", " ")
}
# function to apply to claims data
parse_data <- function(.df){
out <- .df %>%
filter(str_detect(text_tmp, '<!')) %>%
rowwise() %>%
mutate(text_clean = parse_fn(text_tmp)) %>%
unnest(text_clean)
return(out)
}
nlp_fn <- function(parse_data.out){
out <- parse_data.out %>%
unnest_tokens(output = token,
input = text_clean,
token = 'words',
stopwords = str_remove_all(stop_words$word,
'[[:punct:]]')) %>%
mutate(token.lem = lemmatize_words(token)) %>%
filter(str_length(token.lem) > 2) %>%
count(.id, bclass, token.lem, name = 'n') %>%
bind_tf_idf(term = token.lem,
document = .id,
n = n) %>%
pivot_wider(id_cols = c('.id', 'bclass'),
names_from = 'token.lem',
values_from = 'tf_idf',
values_fill = 0)
return(out)
}
nlp_fn_multi <- function(parse_data.out){
out <- parse_data.out %>%
unnest_tokens(output = token,
input = text_clean,
token = 'words',
stopwords = str_remove_all(stop_words$word,
'[[:punct:]]')) %>%
mutate(token.lem = lemmatize_words(token)) %>%
filter(str_length(token.lem) > 2) %>%
count(.id, mclass, token.lem, name = 'n') %>%
bind_tf_idf(term = token.lem,
document = .id,
n = n) %>%
pivot_wider(id_cols = c('.id', 'mclass'),
names_from = 'token.lem',
values_from = 'tf_idf',
values_fill = 0)
return(out)
}
nlp_fn_bigrams <- function(parse_data.out){
out <- parse_data.out %>%
unnest_tokens(output = bigram,
input = text_clean,
token = 'ngrams',
n = 2,
stopwords = str_remove_all(stop_words$word,
'[[:punct:]]')) %>%
separate(bigram, into = c("word1", "word2"), sep = " ") %>%
unite("bigram", word1, word2, sep = " ") %>%
count(.id, bclass, bigram, name = 'n') %>%
bind_tf_idf(term = bigram,
document = .id,
n = n) %>%
pivot_wider(id_cols = c('.id', 'bclass'),
names_from = 'bigram',
values_from = 'tf_idf',
values_fill = 0)
return(out)
}
# Projection functions
projection_fn <- function(.dtm, .prop){
# coerce feature matrix to sparse
dtm_mx <- .dtm %>%
as.matrix() %>%
as('sparseMatrix')
# compute svd
svd_out <- svd(dtm_mx) # svd function: singular value decomposition
# select number of projections
var_df <- tibble(var = svd_out$d^2) %>%
mutate(pc = row_number(),
cumulative = cumsum(var)/sum(var))
n_pc <- which.min(var_df$cumulative < .prop)
# extract loadings
loadings <- svd_out$v[, 1:n_pc] %>% as.matrix()
# extract scores
scores <- (dtm_mx %*% svd_out$v[, 1:n_pc]) %>% as.matrix()
# adjust names
colnames(loadings) <- colnames(scores) <- paste('pc', 1:n_pc, sep = '')
# output
out <- list(n_pc = n_pc,
var = var_df,
projection = loadings,
data = as_tibble(scores))
return(out)
}
reproject_fn <- function(.dtm, .projection_fn_out){
as_tibble(as.matrix(.dtm) %*% .projection_fn_out$projection)
}
prelim1_data = parse_data(claims_raw)
=======
git diff
load('data/claims-raw.RData')
load('data/claims-test.RData')
setwd("/Users/sophia/Library/Mobile Documents/com~apple~CloudDocs/UCSB/4th Year/capstone/module-2-group15")
load('data/claims-test.RData')
load('data/claims-test.RData')
>>>>>>> 430a69f (created multiclass random forest and binary class random forest)
# Import libraries
library(dplyr)
library(tidyr)
library(tidymodels)
library(knitr)
library(tidyverse)
library(tidytext)
library(textstem)
library(rvest)
library(qdapRegex)
library(stopwords)
library(tokenizers)
library(kableExtra)
<<<<<<< HEAD
library(kernlab)
library(Matrix)
library(sparsesvd)
# Import training data
load("~/GitHub/module-2-group15/data/prelim1_data.RData")
head(prelim1_data)
# tokenize the data and select relevant columns
tokenized_data <- nlp_fn_multi(prelim1_data)
# Set class as a factor
tokenized_data$mclass <- as.factor(tokenized_data$mclass)
head(tokenized_data)
# Coerce into sparce DTM Matrix
train_dtm <- tokenized_data %>%
select(-.id, -mclass) %>%
as.matrix() %>%
as("sparseMatrix")
# View the proportion of words that have weight 0
1 - nnzero(train_dtm)/length(train_dtm)
# Split labels and predictors
# train_dtm is the isolated predictors as DTM
# Split labels from predictors
train_labels <- tokenized_data %>%
select(.id, mclass)
# View labels
head(train_labels)
# PCA
# find projections based on training data
dense_dtm <- as.matrix(train_dtm)
# View how many components were used
dense_dtm
# Split labels and predictors
# Isolate predictors
train_dtm <- tokenized_data %>%
select(-.id, -mclass)
# Isolate Labels
train_labels <- tokenized_data %>%
select(.id, mclass)
# View Predictors
head(train_dtm)
# View labels
head(train_labels)
# Split labels and predictors
# Isolate predictors
train_preds <- tokenized_data %>%
select(-.id, -mclass)
# Isolate Labels
train_labels <- tokenized_data %>%
select(.id, mclass)
# View Predictors
head(train_preds)
# View labels
head(train_labels)
install.packages("irlba")
install.packages("factoextra")
# Load necessary libraries
library(caret)
library(irlba)
library(factoextra)
# Step 1: Ensure all columns in train_preds are numeric
train_preds <- as.data.frame(lapply(train_preds, as.numeric))  # Convert all columns to numeric
# Step 2: Remove near-zero variance columns to reduce dimensionality
nzv <- nearZeroVar(train_preds, saveMetrics = TRUE)
# Load necessary libraries
library(caret)
library(irlba)
library(factoextra)
# Step 1: Ensure all columns in train_preds are numeric
train_preds <- as.data.frame(lapply(train_preds, as.numeric))  # Convert all columns to numeric
# Step 2: Remove near-zero variance columns to reduce dimensionality
nzv <- nearZeroVar(train_preds, saveMetrics = TRUE)
train_preds_reduced <- train_preds[, !nzv$nzv]  # Keep only columns with non-zero variance
# Step 3: Convert to a matrix for PCA
train_matrix <- as.matrix(train_preds_reduced)
# Step 4: Perform PCA using the irlba package (efficient for large datasets)
k <- 50  # Number of principal components to compute
pca_irlba <- irlba(train_matrix, nv = k)
# Step 5: Extract and use the principal components
# Principal components matrix (documents in rows, PCs in columns)
principal_components <- pca_irlba$u %*% diag(pca_irlba$d)
# Explained variance for each component
explained_variance <- (pca_irlba$d^2) / sum(pca_irlba$d^2)
cumulative_variance <- cumsum(explained_variance)
principal_components
nrow(principal_components)
ncol(principal_components)
as.data.frame(principal_components)
pc_df <- as.data.frame(principal_components)
# Combine labels with new projected matrix
projected_train_set <- train_labels %>%
transmute(mclass = factor(mclass)) %>%
bind_cols(pc_df)
projected_train_set
save(principal_components, file = 'data/principal_components.RData')
save(principal_components, file ='data/principal_components.RData')
save(principal_components, file ='data/principal_components.RData')
save(principal_components, file ='principal_components.RData')
# Create a recipe for the model
text_recipe <- recipe(mclass ~ ., data=projected_train_set %>% select(-.id)) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_predictors()) %>%
step_zv(all_predictors()) %>%
step_center(all_predictors()) %>%
step_scale(all_predictors()) %>%
step_nzv(all_predictors())
projected_train_set
# Create a recipe for the model
text_recipe <- recipe(mclass ~ ., data=projected_train_set) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_predictors()) %>%
step_zv(all_predictors()) %>%
step_center(all_predictors()) %>%
step_scale(all_predictors()) %>%
step_nzv(all_predictors())
# Define SVM model
svm_model <- svm_rbf(cost = tune(), rbf_sigma = tune()) %>%
set_mode("classification") %>%
set_engine("kernlab")
# Define SVM workflow
svm_wkflow <- workflow() %>%
add_model(svm_model) %>%
add_recipe(text_recipe)
# Define SVM tuning grid
svm_tune_grid <- grid_random(cost(), rbf_sigma(), size = 10)
# Store model results
svm_tuned <- tune_grid(
object=svm_wkflow,
resamples = text_folds,
grid = svm_tune_grid,
metrics = metric_set(roc_auc)
)
# v-fold cross validation
text_folds <- vfold_cv(data=projected_train_set, v=3, strata=mclass)
# Create a recipe for the model
text_recipe <- recipe(mclass ~ ., data=projected_train_set) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_predictors()) %>%
step_zv(all_predictors()) %>%
step_center(all_predictors()) %>%
step_scale(all_predictors()) %>%
step_nzv(all_predictors())
head(projected_train_set)
# Define SVM model
svm_model <- svm_rbf(cost = tune(), rbf_sigma = tune()) %>%
set_mode("classification") %>%
set_engine("kernlab")
# Define SVM workflow
svm_wkflow <- workflow() %>%
add_model(svm_model) %>%
add_recipe(text_recipe)
# Define SVM tuning grid
svm_tune_grid <- grid_random(cost(), rbf_sigma(), size = 10)
# Store model results
svm_tuned <- tune_grid(
object=svm_wkflow,
resamples = text_folds,
grid = svm_tune_grid,
metrics = metric_set(roc_auc)
)
# save svm model
write_rds(svm_tune_res, file = "models/svm.rds")
# save svm model
write_rds(svm_tuned, file = "models/svm.rds")
# load svm model
svm_tuned <- read_rds(file = "models/svm.rds")
best_svm_model <- collect_metrics(svm_tuned) %>%
filter(.metric == "roc_auc") %>%
arrange(desc(mean)) %>%
slice(1)
best_svm_model
# SVM autoplot
autoplot(svm_tuned, metric = 'roc_auc') +
ggtitle("SVM Classifier ROC AUC Tuning Results") +
ylab("Area Under ROC Curve") +
theme_minimal()
# Classification accuracy visualization (TODO)
final_svm_model <- finalize_workflow(svm_wkflow, best_svm_model)
final_svm_model_train <- fit(final_svm_model, tokenized_data)
# Classification accuracy visualization (TODO)
final_svm_model <- finalize_workflow(svm_wkflow, best_svm_model)
final_svm_model_train <- fit(final_svm_model, projected_train_set)
final_svm_model_test <- augment(final_svm_model_train, new_data = claims_test)
# Import testing data
load("~/GitHub/module-2-group15/data/claims-test.RData")
head(claims_test)
# Classification accuracy visualization (TODO)
final_svm_model <- finalize_workflow(svm_wkflow, best_svm_model)
final_svm_model_train <- fit(final_svm_model, projected_train_set)
final_svm_model_test <- augment(final_svm_model_train, new_data = claims_test)
# Classification accuracy visualization (TODO)
final_svm_model <- finalize_workflow(svm_wkflow, best_svm_model)
final_svm_model_train <- fit(final_svm_model, projected_train_set)
# final_svm_model_test <- augment(final_svm_model_train, new_data = claims_test)
# final_svm_model_test
final_svm_model_train
=======
load('data/claims-test.RData')
load('../data/claims-test.RData')
load('../data/claims-raw.RData')
load('/data/claims-test.RData')
load('../data/claims-test.RData')
load('../data/claims-raw.RData')
ls ..
getwd()
token_data = nlp_fn_multi(claims_raw)
# Import libraries
library(dplyr)
library(tidyr)
library(tidymodels)
library(knitr)
library(tidyverse)
library(tidytext)
library(textstem)
library(rvest)
library(qdapRegex)
library(stopwords)
library(tokenizers)
source('scripts/preprocessing.R')
# Import libraries
library(dplyr)
library(tidyr)
library(tidymodels)
library(knitr)
library(tidyverse)
library(tidytext)
library(textstem)
library(rvest)
library(qdapRegex)
library(stopwords)
library(tokenizers)
source('../scripts/preprocessing.R')
source('../scripts/preprocessing.R')
source('../scripts/preprocessing.R')
list.files("../scripts"
list.files("../scripts")
list.files('..')
setwd("/Users/sophia/Library/Mobile Documents/com~apple~CloudDocs/UCSB/4th Year/capstone/module-2-group15")
source('../scripts/preprocessing.R')
token_data = nlp_fn_multi(claims_raw)
parsed_claims = parse_data(claims_raw)
parsed_claims = parse_data(claims_raw)
token_data = nlp_fn_multi(parsed_claims)
token_data$mclass = as.factor(token_data$mclass)
folds = vfold_cv(data=token_data, v=5, strata = mclass)
recipe = recipe(mclass ~ ., data=token_data %>% select(-.id)) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_predictors()) %>%
step_zv(all_predictors()) %>%
step_nzv(all_predictors())
```{r}
rf_model = rand_forest(mtry = tune(),
trees = tune(),
min_n = tune()) %>%
set_engine("ranger", importance = "impurity") %>%
set_mode("classification")
rf_wrkflw = workflow() %>%
add_model(rf_model) %>%
add_recipe(recipe)
rf_grid = grid_regular(mtry(range = c(1, 5)),
trees(range = c(200, 400)),
min_n(range = c(5, 10)),
levels = 4)
rf_tune = tune_grid(
rf_wrkflw,
resamples = pfolds,
grid = rf_grid
)
rf_tune = tune_grid(
rf_wrkflw,
resamples = folds,
grid = rf_grid
)
library(ranger)
install.packages('ranger')
library(ranger)
rf_tune = tune_grid(
rf_wrkflw,
resamples = folds,
grid = rf_grid
)
rf_tune = tune_grid(
rf_wrkflw,
resamples = folds,
grid = rf_grid
)
library(dplyr)
library(tidyr)
library(tidymodels)
library(knitr)
library(tidyverse)
library(ranger)
library(tidytext)
library(textstem)
library(rvest)
library(qdapRegex)
library(stopwords)
library(tokenizers)
parsed_claims = parse_data(claims_raw)
```{r, warning=FALSE, message=FALSE}
# Import libraries
library(dplyr)
library(tidyr)
library(tidymodels)
library(knitr)
library(tidyverse)
library(ranger)
library(tidytext)
library(textstem)
library(rvest)
library(qdapRegex)
library(stopwords)
library(tokenizers)
source('../scripts/preprocessing.R')
source('../scripts/preprocessing.R')
load('../data/claims-test.RData')
source('../scripts/preprocessing.R')
load('../data/claims-raw.RData')
source('../scripts/preprocessing.R')
parsed_claims = parse_data(claims_raw)
token_data = nlp_fn_multi(parsed_claims)
token_data = nlp_fn_multi(parsed_claims)
token_data$mclass = as.factor(token_data$mclass)
folds = vfold_cv(data=token_data, v=3, strata = mclass)
recipe = recipe(mclass ~ ., data=token_data %>% select(-.id)) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_predictors()) %>%
step_zv(all_predictors()) %>%
step_nzv(all_predictors())
rf_model = rand_forest(mtry = tune(),
trees = tune(),
min_n = tune()) %>%
set_engine("ranger", importance = "impurity") %>%
set_mode("classification")
rf_wrkflw = workflow() %>%
add_model(rf_model) %>%
add_recipe(recipe)
rf_grid = grid_regular(mtry(range = c(1, 3)),
trees(range = c(200, 400)),
min_n(range = c(5, 10)),
levels = 4)
rf_tune = tune_grid(
rf_wrkflw,
resamples = folds,
grid = rf_grid
)
rf_tune = tune_grid(
rf_wrkflw,
grid = rf_grid
)
rf_tune = tune_grid(
rf_wrkflw,
resamples = folds,
grid = rf_grid
)
rf_model = rand_forest(mtry = tune(),
min_n = tune(),
trees = 200) %>%
set_engine("ranger", importance = "impurity") %>%
set_mode("classification")
rf_wrkflw = workflow() %>%
add_model(rf_model) %>%
add_recipe(recipe)
rf_grid = grid_random(mtry(range = c(1, 3)),
trees(range = c(200, 400)),
min_n(range = c(5, 10)),
size = 20)
rf_tune = tune_grid(
rf_wrkflw,
resamples = folds,
grid = rf_grid
)
prepped_recipe = recipe %>% prep() %>% bake(new_data = NULL)
rf_model = rand_forest(mtry = tune(),
min_n = tune(),
trees = 200) %>%
set_engine("ranger", importance = "impurity") %>%
set_mode("classification")
rf_wrkflw = workflow() %>%
add_model(rf_model) %>%
add_variables(outcomes = mclass, predictors = names(prepped_recipe))
rf_grid = grid_random(mtry(range = c(1, 3)),
min_n(range = c(5, 10)),
size = 20)
rf_grid = grid_random(mtry(range = c(1, 3)),
min_n(range = c(5, 10)),
size = 20)
```{r}
rf_tune = tune_grid(
rf_wrkflw,
resamples = folds,
grid = rf_grid
)
ncol(prepped_recipe)
rf_wrkflw %>% fit(data = token_data)
glimpse(prepped_recipe)
# Import libraries
library(dplyr)
library(tidyr)
library(tidymodels)
library(knitr)
library(tidyverse)
library(ranger)
library(tidytext)
library(textstem)
library(rvest)
library(qdapRegex)
library(stopwords)
library(tokenizers)
load('../data/claims-test.RData')
load('../data/claims-raw.RData')
source('../scripts/preprocessing.R')
parsed_claims = parse_data(claims_raw)
token_data = nlp_fn_multi(parsed_claims)
token_data$mclass = as.factor(token_data$mclass)
folds = vfold_cv(data=token_data, v=3, strata = mclass)
recipe = recipe(mclass ~ ., data=token_data %>% select(-.id)) %>%
step_dummy(all_nominal_predictors()) %>%
step_nzv(all_predictors(), -all_outcomes())
rf_model = rand_forest(mtry = tune(),
min_n = tune(),
trees = 200) %>%
set_engine("ranger", importance = "impurity") %>%
set_mode("classification")
rf_wrkflw = workflow() %>%
add_model(rf_model) %>%
add_recipe(recipe)
rf_grid = grid_random(mtry(range = c(1, 3)),
min_n(range = c(5, 10)),
size = 20)
resamples = f
rf_tune = tune_grid(
rf_wrkflw,
resamples = folds,
grid = rf_grid
)
save(rf_tune, file = "results/rf-mutliclass.rda")
save(rf_tune, file = "/results/rf-mutliclass.rda")
getwd
getwd()
save(rf_tune, file = "../results/rf-mutliclass.rda")
rf_best = select_best(rf_tune, metric = "roc_auc")
load("rf.rda")
rf_best = select_best(rf_tune, metric = "roc_auc")
(rf_best)
save(rf_tune, file = "../results/rf-multi-mtry=3tree=200min_n=8.rda")
rf_grid = grid_random(mtry(range = c(1, 3)),
trees(c(200, 300, 400)),
min_n(range = c(5, 10)),
size = 20)
rf_grid = grid_random(mtry(range = c(1, 3)),
trees(range = c(200,)),
min_n(range = c(5, 10)),
size = 20)
rf_grid = grid_random(mtry(range = c(1, 3)),
trees(range = c(200,300)),
min_n(range = c(5, 10)),
size = 20)
rf_tune = tune_grid(
rf_wrkflw,
resamples = folds,
grid = rf_grid
)
rf_model = rand_forest(mtry = tune(),
min_n = tune(),
trees = tune()) %>%
set_engine("ranger", importance = "impurity") %>%
set_mode("classification")
rf_wrkflw = workflow() %>%
add_model(rf_model) %>%
add_recipe(recipe)
rf_grid = grid_random(mtry(range = c(1, 3)),
trees(range = c(200,300)),
min_n(range = c(5, 10)),
size = 20)
rf_grid = grid_random(mtry(range = c(1, 3)),
trees(range = c(200,300)),
min_n(range = c(5, 10)),
size = 20)
```{r}
rf_tune = tune_grid(
rf_wrkflw,
resamples = folds,
grid = rf_grid
)
rf_best = select_best(rf_tune, metric = "roc_auc")
(rf_best)
autoplot(rf_tune) + theme_minimal()
load("../results/rf-multiclass.rda")
rf_best = select_best(rf_tune, metric = "roc_auc")
autoplot(rf_tune) + theme_minimal()
rf_best = select_best(rf_tune, metric = "roc_auc")
(rf_best)
rf_final = finalize_workflow(rf_wrkflw, rf_best)
rf_final = fit(rf_final, token_data)
prep(recipe) %>% bake(token_data)
rf_final = finalize_workflow(rf_wrkflw, rf_best)
rf_final = fit(rf_final, token_data)
print(rf_best)
rf_model = rand_forest(mtry = tune(),
min_n = tune(),
trees = tune()) %>%
set_engine("ranger", importance = "impurity") %>%
set_mode("classification")
rf_wrkflw = workflow() %>%
add_model(rf_model) %>%
add_recipe(recipe)
rf_grid = grid_random(mtry(range = c(1, 3)),
trees=(range = c(200, 300)),
min_n(range = c(5, 10)),
size = 20)
rf_grid = grid_random(mtry(range = c(1, 3)),
trees=(range = c(200, 300)),
min_n(range = c(5, 10)),
size = 20)
rf_grid = grid_random(
mtry(range = c(1, 3)),
trees(range = c(200, 400)),
min_n(range = c(5, 10)),
size = 20
)
rf_tune = tune_grid(
rf_wrkflw,
resamples = folds,
grid = rf_grid
)
save(rf_tune, file = "../results/rf-mutliclass.rda")
load("../results/rf-multiclass.rda")
autoplot(rf_tune) + theme_minimal()
rf_best = select_best(rf_tune, metric = "roc_auc")
(rf_best)
# rf_tune = tune_grid(
#   rf_wrkflw,
#   resamples = folds,
#   grid = rf_grid
# )
#
save(rf_tune, file = "../results/rf-mutliclass.rda")
load("../results/rf-multiclass.rda")
autoplot(rf_tune) + theme_minimal()
load("../results/rf-multiclass.rda")
load("/results/rf-multiclass.rda")
autoplot(rf_tune) + theme_minimal()
rf_best = select_best(rf_tune, metric = "roc_auc")
(rf_best)
rf_model = rand_forest(mtry = tune(),
min_n = tune(),
trees = tune()) %>%
set_engine("ranger", importance = "impurity") %>%
set_mode("classification")
# Import libraries
library(dplyr)
library(tidyr)
library(tidymodels)
library(knitr)
library(tidyverse)
library(ranger)
library(tidytext)
library(textstem)
library(rvest)
library(qdapRegex)
library(stopwords)
library(tokenizers)
load('../data/claims-test.RData')
load('../data/claims-raw.RData')
source('../scripts/preprocessing.R')
test_parsed = parse_data(claims_test)
test_data = nlp_fn_multi(test_parsed)
test_parsed = parse_data(claims_test)
test_data = nlp_fn_multi(test_parsed)
glimpse(test_data)
glimpse(test_parsed)
parsed_claims = parse_data(claims_raw)
parsed_claims = parse_data(claims_raw)
token_data = nlp_fn_multi(parsed_claims)
token_data$mclass = as.factor(token_data$mclass)
folds = vfold_cv(data=token_data, v=3, strata = mclass)
recipe = recipe(mclass ~ ., data=token_data %>% select(-.id)) %>%
step_dummy(all_nominal_predictors()) %>%
step_nzv(all_predictors(), -all_outcomes())
rf_model = rand_forest(mtry = tune(),
min_n = tune(),
trees = tune()) %>%
set_engine("ranger", importance = "impurity") %>%
set_mode("classification")
rf_wrkflw = workflow() %>%
add_model(rf_model) %>%
add_recipe(recipe)
rf_grid = grid_random(
mtry(range = c(1, 3)),
trees(range = c(200, 400)),
min_n(range = c(5, 10)),
size = 20
)
rf_tune = tune_grid(
rf_wrkflw,
resamples = folds,
grid = rf_grid
)
save(rf_tune, file = "../results/rf-mutliclass.rda")
save(rf_tune, file = "../results/rf-mutliclass.rda")
autoplot(rf_tune) + theme_minimal()
rf_best = select_best(rf_tune, metric = "roc_auc")
rf_best = select_best(rf_tune, metric = "roc_auc")
(rf_best)
rf_final = finalize_workflow(rf_wrkflw, rf_best)
rf_final = fit(rf_final, token_data)
rf_final %>% extract_fit_parsnip() %>%
vip() +
theme_minimal()
load("../results/rf-multiclass.rda")
# rf_tune = tune_grid(
#   rf_wrkflw,
#   resamples = folds,
#   grid = rf_grid
# )
#
save(rf_tune, file = "../results/rf-mutliclass.rds")
load("../results/rf-multiclass.rds")
autoplot(rf_tune) + theme_minimal()
load("../results/rf-multiclass.rds")
# rf_tune = tune_grid(
#   rf_wrkflw,
#   resamples = folds,
#   grid = rf_grid
# )
#
save(rf_tune, file = "../results/rf-mutliclass.rda")
load("../results/rf-multiclass.rda")
getwd()
load("~/Library/Mobile Documents/com~apple~CloudDocs/UCSB/4th Year/capstone/module-2-group15/results/rf-mutliclass.rda")
View(rf_model)
View(rf_grid)
View(rf_model)
View(rf_grid)
library(knitr)
# Import libraries
library(dplyr)
library(tidyr)
library(tidymodels)
library(knitr)
library(tidyverse)
library(ranger)
library(tidytext)
library(textstem)
library(rvest)
library(qdapRegex)
library(stopwords)
library(tokenizers)
load('../data/claims-test.RData')
load('../data/claims-raw.RData')
source('../scripts/preprocessing.R')
parsed_claims = parse_data(claims_raw)
token_data = nlp_fn_multi(parsed_claims)
token_data$mclass = as.factor(token_data$mclass)
folds = vfold_cv(data=token_data, v=3, strata = mclass)
recipe = recipe(mclass ~ ., data=token_data %>% select(-.id)) %>%
step_dummy(all_nominal_predictors()) %>%
step_nzv(all_predictors(), -all_outcomes())
rf_model = rand_forest(mtry = tune(),
min_n = tune(),
trees = tune()) %>%
set_engine("ranger", importance = "impurity") %>%
set_mode("classification")
rf_wrkflw = workflow() %>%
add_model(rf_model) %>%
add_recipe(recipe)
rf_grid = grid_random(
mtry(range = c(1, 3)),
trees(range = c(200, 400)),
min_n(range = c(5, 10)),
size = 20
)
load("../results/rf-multiclass.rda")
load("~/Library/Mobile Documents/com~apple~CloudDocs/UCSB/4th Year/capstone/module-2-group15/results/rf-mutliclass.rda")
autoplot(rf-multiclass) + theme_minimal()
rf_tune
autoplot(rf_tune) + theme_minimal()
rf_best = select_best(rf_tune, metric = "roc_auc")
(rf_best)
rf_final = finalize_workflow(rf_wrkflw, rf_best)
rf_final = fit(rf_final, token_data)
rf_final = fit(rf_final, token_data)
rf_final %>% extract_fit_parsnip() %>%
vip() +
theme_minimal()
pwd
list.files
>>>>>>> 430a69f (created multiclass random forest and binary class random forest)
