---
title: "Deliverable 2"
format: html
editor: visual
---

## Methods

#### Data Acquisition and Pre-processing

For our models, we chose to run Random Forests, a Support Vector Machine (SVM), and a Neural Network. To prepare the raw HTML data, we employed two different approaches for each predictive model. Both the Random Forest and SVM models utilized data scraped from paragraph content along with corresponding headers. In contrast, the Neural Network model leveraged a more enriched dataset that included paragraph content, headers, and span elements. This modification for the Neural Network was adopted after observing improved performance when span data was included. Interestingly, this was not true for the other models.

All raw HTML data were then preprocessed using a natural language processing (NLP) function. This function cleaned the data by removing HTML tags, stripping punctuation, and eliminating stopwords. The cleaned data was then tokenized to create a format suitable for the machine learning models. However, none of our final models used any bigram or ngram data when training, as the observed performance did not change significantly.

#### Model Setup and Dependencies

Each of our models utilized different approaches to optimize performance based on experimental findings. The Random Forest and SVM models used dimensionality reduction through Principal Component Analysis (PCA), while the Neural Network used vectorization for improved learning.

For both of the Random Forest models (binary and multi-class), the tokenized data was divided into training and testing sets. PCA was then applied in the recipe to reduce dimensionality, using 100 components. Predictors with zero or near-zero variance were dropped, and 3-fold cross-validation was implemented. Finally, the hyperparameters were tuned to find the best model configuration.

For the SVM (multi-class) model, PCA was perfomed on the tokenized data directly, using only 50 components, as it was found that more components did not yield significant accuracy improvements. This projected data was then split into testing and training sets. Similar to the Random Forest models, predictors with zero or near-zero variance were dropped, 3-fold cross-validation was used, followed by hyperparameter tuning to refine the model's performance.

Finally, the Neural Network (binary) model was trained using tokenized data processed through vectorization, with an ouput dimension of 128. Dimensionality was reduced with two convolutional layers, each with 128 filters, a kernel size of 5, and ReLU activation, with max pooling layers of 5. After the vectorized data was split into testing and training, we used the Adam optimization algorithm, a binary cross-entropy loss function, and a single hidden layer comprising of 64 nodes. A dropout layer with a rate of 0.5 was added to prevent overfitting.

## Findings

The Random Forest model had the highest accuracy for binary classification at 81%, narrowly outperforming the Neural Network's binary classification accuracy of 78%. For multi-class classification, the Random Forest achieved 79% accuracy, surpassing the Support Vector Machine, which only had 73%. Overall, the best model for both the binary and multi-class classification was the Random Forest.