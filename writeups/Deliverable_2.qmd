---
title: "Deliverable 2"
format: html
editor: visual
---

## Question (Placeholder)

-   ***Deliverable 2:*** write a 1-page summary of methods and findings for task 4. This should describe text preprocessing, your predictive models, and estimated prediction accuracy for each model. Store the rendered document in the `writeups` directory.

## Methods

#### Data Acquisition and Pre-processing

To prepare the raw HTML data, we employed two different approaches for each predictive model. Both the Random Forest and Support Vector Machine (SVM) models utilized data scraped from paragraph content along with corresponding headers. In contrast, the Neural Network model leveraged a more enriched dataset that included paragraph content, headers, and span elements. This modification for the Neural Network was adopted after observing improved performance when span data was included. Interestingly, this was not true for the other models.

All raw HTML data were then preprocessed using a natural language processing (NLP) function. This function cleaned the data by removing HTML tags, stripping punctuation, and eliminating stopwords. The cleaned data was then tokenized to create a format suitable for the machine learning models.

#### Model Setup and Dependencies

Each of our models utilized different approaches to optimize performance based on experimental findings. The Random Forest and SVM models used dimensionality reduction through Principal Component Analysis (PCA), while the Neural Network used vectorization for improved learning.

For both of the Random Forest models (binary and multi-class), the tokenized data was divided into training and testing sets. PCA was then applied in the recipe to reduce dimensionality, using 100 components. Predictors with zero or near-zero variance were dropped, and 3-fold cross-validation was implemented. Finally, the hyperparameters were tuned to find the best model configuration.

For the SVM (multi-class) model, PCA was perfomed on the tokenized data directly, using only 50 components, as it was found that more components did not yield significant accuracy improvements. This projected data was then split into testing and training sets. Similar to the Random Forest models, predictors with zero or near-zero variance were dropped, 3-fold cross-validation was used, followed by hyperparameter tuning to refine the model's performance.

Finally, the Neural Network model was trained using tokenized data processed through vectorization, with an ouput dimension of 128. Dimensionality was reduced with two convolutional layers, each with 128 filters, a kernel size of 5, and ReLU activation, with max pooling layers of 5. After the vectorized data was split into testing and training, we used the Adam optimization algorithm, a binary cross-entropy loss function, and a single hidden layer comprising of 64 nodes. A dropout layer with a rate of 0.5 was added to prevent overfitting.

## Findings

-   Model results (did binary do better than multi-class?, explain each model's results, explain that interpretation does not matter in this classification problem - we can rely on the accuracies)

-   Best model(s), training accuracy, final thoughts, anything that could've been done better (include bigrams, ngrams couldve changed results)
